{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24429,
     "status": "ok",
     "timestamp": 1751562913052,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "5ff24c23",
    "outputId": "813f9395-6400-4e5a-9f19-a99a801bee40"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, validation_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install imbalanced-learn\n",
    "#!pip install gplearn\n",
    "#!pip install dice-ml\n",
    "!pip install lime\n",
    "!pip install shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "0TgO42BeiPu7"
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "v5IGLlo9NsX8"
   },
   "source": [
    "DATA Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13237,
     "status": "ok",
     "timestamp": 1751562926309,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "cW5sjpK8Nne1",
    "outputId": "d5f51af3-dc18-4cc7-f2c1-5b81c4e024cc"
   },
   "outputs": [],
   "source": [
    "#Extracting sheets,removing unneccessary cells,set dates format\n",
    "\n",
    "# Load sheets\n",
    "data_training = pd.read_excel(\"Training dataset.20150707.xlsx\", sheet_name='Training dataset')\n",
    "data_inspection = pd.read_excel(\"Training dataset.20150707.xlsx\", sheet_name='INSPECTION_RUN')\n",
    "data_tonnage = pd.read_excel(\"Training dataset.20150707.xlsx\", sheet_name='TONNAGE_SAMPLE_DATA')\n",
    "\n",
    "# Convert date columns\n",
    "data_training['TEST_DT'] = pd.to_datetime(data_training['TEST_DT'], format='%Y-%m-%d', errors='coerce')\n",
    "data_inspection['TEST_DT'] = pd.to_datetime(data_inspection['TEST_DT'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Remove header rows\n",
    "data_training = data_training.iloc[26:, :]\n",
    "data_inspection = data_inspection.iloc[3:, :]\n",
    "data_tonnage = data_tonnage.iloc[4:, :]\n",
    "\n",
    "# Remove duplicates from training data\n",
    "data_training = data_training.sort_values(by='DEF_AMPLTD', ascending=True)\n",
    "data_training = data_training.drop_duplicates(\n",
    "    subset=['MILEPOST', 'LINE_SEG_NBR', 'TRACK_SDTK_NBR', 'TEST_DT', 'GEO_CAR_NME', 'TSC_CD'],\n",
    "    keep='last'\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Remove duplicates from tonnage data\n",
    "data_tonnage = data_tonnage.sort_values(by='TOT_DFLT_MGT', ascending=True)\n",
    "data_tonnage = data_tonnage.drop_duplicates(\n",
    "    subset=['LINE_SEG_NBR', 'TRACK_SDTK_NBR', 'YEAR', 'MONTH', 'MILEPOST_START', 'MILEPOST_END'],\n",
    "    keep='last'\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# data training\n",
    "##display('data_training:')\n",
    "##display(data_training)\n",
    "print()\n",
    "# data inspection\n",
    "##display('data_inspection:')\n",
    "##display(data_inspection)\n",
    "print()\n",
    "# data tonnage\n",
    "##display('data_tonnage:')\n",
    "##display(data_tonnage)\n",
    "print()\n",
    "\n",
    "################################################################################################################################################################################################################################################################################################\n",
    "# variables defenition\n",
    "##display(\" variables Definitions:\" )\n",
    "##display(\"\"\"***LINE_SEG_NBR: Integer :Every track on the railroad has a unique identifying line segment number. Could be single or double tracks. Using line segment (LINE_SEG_NBR) and mile post (MILPOST_START and MILEPOST_END) you can identify any location on the system.\"\"\")\n",
    "##print()\n",
    "##display(\"\"\"***TRACK_SDTK_NBR: CHARACTER : Distingish indivuidaul track segments. Mainline & branch numbers: 0=SINGLE TRACK, 1-9=MULTIPLE MAIN LINES (For example, 1=NORTH MAIN, 2=SOUTH MAIN). Tracks outside of main/branch are referred to as side tracks. 5=SIDING TRACK\n",
    "##\"\"\")\n",
    "##print()\n",
    "##display(\"***TEST_DT: DATE : The date on which testing was performed.\")\n",
    "##print()\n",
    "##display( \"***DEF_NBR:INTEGER:Defect number. Every defect detetected by a Gemoetry car gets a unique id.\")\n",
    "##print()\n",
    "##display( \"***GEO_CAR_NME:CHARACTER:Geometry cars names. Examples of names include: GEO105, GEO505  ETC.\")\n",
    "##print()\n",
    "##display( \"***DEF_PRTY:CHARACTER:Yellow or red.\")\n",
    "##print()\n",
    "##display( \"***DEF_LGTH:INTEGER:Length of defect in feet, as reported by the measurement car.\")\n",
    "##print()\n",
    "##display( \"***DEF_AMPLTD:DECIMAL:Defect amplitude -- maximum size of defect in inches or degrees within defect length.\")\n",
    "##print()\n",
    "##display( \"***TSC_CD:CHARACTER:Track codes including tangent, spiral and curve.\")\n",
    "##print()\n",
    "##display( \"***CLASS:CHARACTER:Class of tracks. All tracks get a number between one and five. Each class represents operating speed limits for passenger and freight traffic. Class one has the lowest speed limit and class five has the highest speed limit.\")\n",
    "##print()\n",
    "##display( \"***TEST_FSPD:CHARACTER:Operating speed for freight trains.\")\n",
    "##print()\n",
    "##display( \"***TEST_PSPD:CHARACTER:Operating speed for passenger trains. If the value = 0, then it means that it does not have passenger traffic.\")\n",
    "##print()\n",
    "##display( \"***DFCT_TYPE:Defect type--the geormetric defect type such as XLEVEL, SURFACE, DIP....\")\n",
    "##print()\n",
    "##display(\"***YEAR:SMALLINT:Year for which tonnage is accumulated for.\")\n",
    "##print()\n",
    "##display(\"***MONTH:CHARACTER:The month the tonnage was accumulated for this train. Sum of all the the daily mileage for this month.\")\n",
    "##print()\n",
    "##display(\"***TOT_CAR_EAST/WEST:INTEGER:Total number of cars traveling east/West.\")\n",
    "##print()\n",
    "##display(\"***TOT_TRN_EAST/WEST:INTEGER:Total number of trains traveling east/West.\")\n",
    "##print()\n",
    "##display(\"***TOT_DFLT_MGT:DECIMAL:Sum of total gross tons traveling across the section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "g_02LqUZNnpF"
   },
   "outputs": [],
   "source": [
    "#Preparing the datasets for matching\n",
    "\n",
    "# Extract year from test date\n",
    "data_training['YEAR'] = data_training['TEST_DT'].dt.year.astype('int64')\n",
    "data_tonnage['YEAR'] = data_tonnage['YEAR'].astype('int64')\n",
    "\n",
    "# Convert milepost columns to numeric and calculate midpoint\n",
    "data_tonnage['MILEPOST_START'] = pd.to_numeric(data_tonnage['MILEPOST_START'], errors='coerce')\n",
    "data_tonnage['MILEPOST_END'] = pd.to_numeric(data_tonnage['MILEPOST_END'], errors='coerce')\n",
    "data_tonnage['MILEPOST'] = (data_tonnage['MILEPOST_START'] + data_tonnage['MILEPOST_END']) / 2\n",
    "\n",
    "# Display cleaned data\n",
    "#display(data_tonnage)\n",
    "#display(data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "FCokswosOA61"
   },
   "outputs": [],
   "source": [
    "# Merging tonnage and training\n",
    "\n",
    "# Prepare milepost columns for merging\n",
    "data_tonnage['MILEPOST'] = pd.to_numeric(data_tonnage['MILEPOST'], errors='coerce').astype('int64')\n",
    "data_training['MILEPOST'] = pd.to_numeric(data_training['MILEPOST'], errors='coerce').astype('int64')\n",
    "\n",
    "# Sort by milepost\n",
    "data_training = data_training.sort_values('MILEPOST')\n",
    "data_tonnage = data_tonnage.sort_values('MILEPOST')\n",
    "\n",
    "# Rename to avoid conflicts during merge\n",
    "data_training.rename(columns={'MILEPOST': 'MILEPOST_training'}, inplace=True)\n",
    "data_tonnage.rename(columns={'MILEPOST': 'MILEPOST_tonnage'}, inplace=True)\n",
    "\n",
    "# Merge using merge_asof\n",
    "merged_data = pd.merge_asof(\n",
    "    data_training,\n",
    "    data_tonnage,\n",
    "    left_on='MILEPOST_training',\n",
    "    right_on='MILEPOST_tonnage',\n",
    "    by=['LINE_SEG_NBR', 'TRACK_SDTK_NBR', 'YEAR'],\n",
    "    direction='nearest',\n",
    "    suffixes=('_training', '_tonnage')\n",
    ")\n",
    "\n",
    "# Clean up columns\n",
    "merged_data.drop(columns=['Variable Name_tonnage'], inplace=True)\n",
    "merged_data.rename(columns={\n",
    "    'Variable Name_training': 'Variable Name',\n",
    "    'MILEPOST_training': 'MILEPOST'\n",
    "}, inplace=True)\n",
    "merged_data.drop(columns=['MILEPOST_tonnage'], inplace=True)\n",
    "\n",
    "# Define segment ID\n",
    "merged_data['seg_id'] = (\n",
    "    merged_data['LINE_SEG_NBR'].astype(str) + '-' +\n",
    "    merged_data['TRACK_SDTK_NBR'].astype(str) + '-' +\n",
    "    merged_data['MILEPOST_START'].astype(str) + '-' +\n",
    "    merged_data['MILEPOST_END'].astype(str)\n",
    ")\n",
    "merged_data.insert(0, 'seg_id', merged_data.pop('seg_id'))\n",
    "\n",
    "# Filter by milepost range with tolerance\n",
    "tolerance = 0.1\n",
    "filtered_data_XAI = merged_data[\n",
    "    (merged_data['MILEPOST'] >= merged_data['MILEPOST_START'] * (1 - tolerance)) &\n",
    "    (merged_data['MILEPOST'] <= merged_data['MILEPOST_END'] * (1 + tolerance))\n",
    "]\n",
    "\n",
    "# Export\n",
    "filtered_data_XAI.to_excel('filtered_data_XAI.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "fbf92805-bc1b-4d78-aa3f-1bce71e6712d"
   },
   "source": [
    "# Prediction: Machine-learning\n",
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "5648cf45-0725-479a-9753-6a14473b6376"
   },
   "source": [
    "Gradient Boosting  >> OK <<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17461,
     "status": "ok",
     "timestamp": 1751562953203,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "d119e239-ce19-4c36-a153-21a6c690f72e",
    "outputId": "6ef4f737-3818-49c0-8950-997ff4fab43b"
   },
   "outputs": [],
   "source": [
    "#ML #Gradient Boosting + SMOTE\n",
    "# Preparing whole data for training\n",
    "# Quantifying values in the \"DEF_PRTY\",\"TSC_CD\" and \"DFCT_TYPE\" column\n",
    "\n",
    "filtered_data_grboost = pd.read_excel(\"filtered_data_XAI.xlsx\")\n",
    "filtered_data_grboost['DEF_PRTY'] = filtered_data_grboost['DEF_PRTY'].replace({'YEL': 0, 'RED': 1})\n",
    "filtered_data_grboost['TSC_CD'] = filtered_data_grboost['TSC_CD'].replace({'T': 0, 'C': 1})\n",
    "filtered_data_grboost['DFCT_TYPE'] = filtered_data_grboost['DFCT_TYPE'].replace({'XLEVEL': 1, 'DIP': 2, 'SURFACE': 3})\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# remove outliers\n",
    "selected_cols_98 = ['DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "percentile_98 = filtered_data_grboost[selected_cols_98].quantile(0.98)\n",
    "filtered_data_grboost = filtered_data_grboost[\n",
    "    (filtered_data_grboost[selected_cols_98] <= percentile_98).all(axis=1)\n",
    "]\n",
    "\n",
    "# Features and target\n",
    "#X_train = filtered_data_grboost[['MONTH','YEAR','DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']]\n",
    "X_raw = filtered_data_grboost[['DEF_LGTH','TEST_FSPD','TSC_CD','TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "#X_raw = filtered_data_grboost[['DEF_LGTH','TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "Y_train = filtered_data_grboost['DEF_PRTY']\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_raw), columns=X_raw.columns)\n",
    "\n",
    "#  30% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "model_grboost = GradientBoostingClassifier()\n",
    "\n",
    "# model training\n",
    "model_grboost.fit(X_train, Y_train)\n",
    "\n",
    "# Validation accuracy\n",
    "accuracy_val_grboost = model_grboost.score(X_val, Y_val)\n",
    "print(\"Validation Accuracy:\", accuracy_val_grboost)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_scores_grboost = cross_val_score(model_grboost, X_train, Y_train, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_grboost)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_grboost))\n",
    "\n",
    "# Evaluattion\n",
    "Y_pred_grboost = model_grboost.predict(X_test)\n",
    "accuracy_test_grboost = accuracy_score(Y_test, Y_pred_grboost)\n",
    "precision_grboost = precision_score(Y_test, Y_pred_grboost)\n",
    "recall_grboost = recall_score(Y_test, Y_pred_grboost)\n",
    "f1_grboost = f1_score(Y_test, Y_pred_grboost)\n",
    "roc_auc_grboost = roc_auc_score(Y_test, Y_pred_grboost)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_test_grboost)\n",
    "print(\"Precision:\", precision_grboost)\n",
    "print(\"Recall:\", recall_grboost)\n",
    "print(\"F1-Score:\", f1_grboost)\n",
    "print(\"ROC AUC Score:\", roc_auc_grboost)\n",
    "\n",
    "\n",
    "import joblib\n",
    "joblib.dump(model_grboost, 'model_grboost.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 28273,
     "status": "ok",
     "timestamp": 1751562981482,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "5Jh-WM7yiGWe",
    "outputId": "c073cfc0-4150-4a01-9128-ea973d75a88a"
   },
   "outputs": [],
   "source": [
    "# --- XAI Tools Gragient Boosting ----\n",
    "# IAL: Immediate action limits\n",
    "# IL:  Intervention limits\n",
    "\n",
    "# 1. LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer_lime_grboost= lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['IL','IA'],\n",
    "     mode='classification')\n",
    "\n",
    "i = 0  # instance\n",
    "exp_grboost = explainer_lime_grboost.explain_instance(X_test.iloc[i], model_grboost.predict_proba)\n",
    "exp_grboost.show_in_notebook(show_table=True)\n",
    "\n",
    "joblib.dump(exp_grboost, \"explainer_lime_grboost.joblib\")\n",
    "\n",
    "# 2. SHAP\n",
    "import shap\n",
    "\n",
    "explainer_shap_grboost = shap.Explainer(model_grboost.predict, X_train)\n",
    "shap_values_grboost = explainer_shap_grboost(X_test[:1000])\n",
    "\n",
    "shap.plots.waterfall(shap_values_grboost[0])  # Local explanation\n",
    "shap.plots.beeswarm(shap_values_grboost)      # Global importance\n",
    "shap.plots.heatmap(shap_values_grboost)\n",
    "shap.plots.bar(shap_values_grboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "VFXQvDMT3Jo4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate LIME plot\n",
    "fig_lime = exp_grboost.as_pyplot_figure()\n",
    "fig_lime.savefig(\"lime_explanation_grboost.jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_lime)\n",
    "\n",
    "# Generate SHAP plots\n",
    "shap.plots.waterfall(shap_values_grboost[0], show=False)\n",
    "plt.savefig(\"shap_waterfall_grboost.jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "shap.plots.beeswarm(shap_values_grboost, show=False)\n",
    "plt.savefig(\"shap_beeswarm_grboost.jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "shap.plots.heatmap(shap_values_grboost, show=False)\n",
    "plt.savefig(\"shap_heatmap_grboost.jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "shap.plots.bar(shap_values_grboost, show=False)\n",
    "plt.savefig(\"shap_bar_grboost.jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "44400da7-8038-4585-a526-b149e8c45da6"
   },
   "source": [
    "XGBoost >>OK<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8044,
     "status": "ok",
     "timestamp": 1751562991778,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "73c04c88-d657-4a01-b2c7-33e253a721af",
    "outputId": "2b9adb0a-5cc2-43e1-ebb2-6699a5d500ec"
   },
   "outputs": [],
   "source": [
    "#ML #XGBoost+SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "filtered_data_xgboost= pd.read_excel(\"filtered_data_XAI.xlsx\")\n",
    "\n",
    "# categorical columns to numeric\n",
    "categorical_columns = ['MONTH','DEF_AMPLTD', 'DEF_LGTH', 'TEST_FSPD', 'TEST_PSPD', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "filtered_data_xgboost[categorical_columns] = filtered_data_xgboost[categorical_columns].apply(pd.to_numeric)\n",
    "\n",
    "categorical_columns = ['DEF_PRTY', 'TSC_CD', 'DFCT_TYPE']\n",
    "for col in categorical_columns:\n",
    "    filtered_data_xgboost[col] = filtered_data_xgboost[col].replace({'YEL': 0, 'RED': 1, 'T': 0, 'C': 1, 'XLEVEL': 1, 'DIP': 2, 'SURFACE': 3})\n",
    "\n",
    "#  remove outliers\n",
    "selected_cols_98 = ['DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "percentile_98 = filtered_data_xgboost[selected_cols_98].quantile(0.98)\n",
    "filtered_data_xgboost = filtered_data_xgboost[\n",
    "    (filtered_data_xgboost[selected_cols_98] <= percentile_98).all(axis=1)\n",
    "]\n",
    "\n",
    "# Features and target\n",
    "#X_train = filtered_data_xgboost[['MONTH','YEAR','DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']]\n",
    "X_train =filtered_data_xgboost[['DEF_LGTH','TEST_FSPD','TSC_CD','TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "#X_train =filtered_data_xgboost[['DEF_LGTH','TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "Y_train = filtered_data_xgboost['DEF_PRTY']\n",
    "\n",
    "# and 30 % test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# XGBoost classifier\n",
    "xgboost_model = xgb.XGBClassifier()\n",
    "\n",
    "# Train the modelt\n",
    "xgboost_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "Y_val_pred = xgboost_model.predict(X_val)\n",
    "accuracy_val_xgboost = accuracy_score(Y_val, Y_val_pred)\n",
    "print(\"Validation Accuracy:\", accuracy_val_xgboost)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores_xgboost = cross_val_score(xgboost_model, X_train, Y_train, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_xgboost)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_xgboost))\n",
    "\n",
    "# Evaluateion\n",
    "Y_pred = xgboost_model.predict(X_test)\n",
    "accuracy_test_xgboost = accuracy_score(Y_test, Y_pred)\n",
    "precision_xgboost = precision_score(Y_test, Y_pred)\n",
    "recall_xgboost = recall_score(Y_test, Y_pred)\n",
    "f1_xgboost = f1_score(Y_test, Y_pred)\n",
    "roc_auc_xgboost = roc_auc_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_test_xgboost)\n",
    "print(\"Precision:\", precision_xgboost)\n",
    "print(\"Recall:\", recall_xgboost)\n",
    "print(\"F1-Score:\", f1_xgboost)\n",
    "print(\"ROC AUC Score:\", roc_auc_xgboost)\n",
    "\n",
    "# Save\n",
    "joblib.dump(xgboost_model, 'xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4842,
     "status": "ok",
     "timestamp": 1751562996624,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "EdcJ8m3HigEm",
    "outputId": "97882bd3-1408-4307-baa9-af6335ed1f13"
   },
   "outputs": [],
   "source": [
    "# --- XAI Tools  XGBoost ---\n",
    "# IAL: Immediate action limits\n",
    "# IL:  Intervention limits\n",
    "\n",
    "# 1. LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer_lime_xgb = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['IL','IA'],\n",
    "    mode='classification'\n",
    ")\n",
    "i = 0  # instance\n",
    "exp_xgb = explainer_lime_xgb.explain_instance(X_test.iloc[i], xgboost_model.predict_proba)\n",
    "exp_xgb.show_in_notebook(show_table=True)\n",
    "\n",
    "joblib.dump(exp_grboost, \"explainer_lime_xgb.joblib\")\n",
    "\n",
    "# 2. SHAP\n",
    "import shap\n",
    "\n",
    "explainer_shap_xgb = shap.Explainer(xgboost_model, X_train)\n",
    "shap_values_xgb = explainer_shap_xgb(X_test[:1000])\n",
    "\n",
    "shap.plots.waterfall(shap_values_xgb[0])  # Local explanation\n",
    "shap.plots.beeswarm(shap_values_xgb)      # Global importance\n",
    "shap.plots.bar(shap_values_xgb)\n",
    "shap.plots.heatmap(shap_values_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "1NauDzft5uDS"
   },
   "outputs": [],
   "source": [
    "# Generate LIME plot\n",
    "fig_lime = exp_grboost.as_pyplot_figure()\n",
    "fig_lime.savefig(\"lime_explanation_xgb.jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_lime)\n",
    "\n",
    "# Save SHAP\n",
    "shap.plots.beeswarm(shap_values_xgb, show=False)\n",
    "plt.savefig(\"shap_beeswarm_xgb.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "shap.plots.waterfall(shap_values_xgb[0], show=False)\n",
    "plt.savefig(\"shap_waterfall_xgb.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "shap.plots.bar(shap_values_xgb, show=False)\n",
    "plt.savefig(\"shap_bar_xgb.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "shap.plots.heatmap(shap_values_xgb, show=False)\n",
    "plt.savefig(\"shap_heatmap_xgb.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "197a8ce2-d50c-450f-8f77-d469938bc636"
   },
   "source": [
    "Random Forest >>OK<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16972,
     "status": "ok",
     "timestamp": 1751563015774,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "103af33b-6732-4306-a3eb-b565b62db9d7",
    "outputId": "663eedcc-65ae-449b-8799-234899345612"
   },
   "outputs": [],
   "source": [
    "#ML #Random Forest+SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# MASK\n",
    "filtered_data_rndforest = pd.read_excel(\"filtered_data_XAI.xlsx\")\n",
    "\n",
    "filtered_data_rndforest['DEF_PRTY'] = filtered_data_rndforest['DEF_PRTY'].replace({'YEL': 0, 'RED': 1})\n",
    "filtered_data_rndforest['TSC_CD'] = filtered_data_rndforest['TSC_CD'].replace({'T': 0, 'C': 1})\n",
    "filtered_data_rndforest['DFCT_TYPE'] = filtered_data_rndforest['DFCT_TYPE'].replace({'XLEVEL': 1, 'DIP': 2, 'SURFACE': 3})\n",
    "\n",
    "#  98th percentile to remove outliers\n",
    "selected_cols_98 = ['DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "percentile_98 = filtered_data_rndforest[selected_cols_98].quantile(0.98)\n",
    "filtered_data_rndforest = filtered_data_rndforest[\n",
    "    (filtered_data_rndforest[selected_cols_98] <= percentile_98).all(axis=1)\n",
    "]\n",
    "\n",
    "#  X_train and Y_train\n",
    "# Features and target\n",
    "#X_train = filtered_data_rndforest[['MONTH','YEAR','DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']]\n",
    "X_raw = filtered_data_rndforest[['DEF_LGTH','TEST_FSPD','TSC_CD','TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "#X_raw = filtered_data_rndforest[['DEF_LGTH','TEST_PSPD', 'DFCT_TYPE','TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "Y_train = filtered_data_rndforest['DEF_PRTY']\n",
    "\n",
    "# Apply StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "X_train = pd.DataFrame(X_scaled, columns=X_raw.columns)  # Preserve column names\n",
    "\n",
    "# split data 30% test\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "#  Random Forest Classifier model\n",
    "model_rndforest = RandomForestClassifier()\n",
    "\n",
    "# Train\n",
    "model_rndforest.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "accuracy_val_rndforest = model_rndforest.score(X_val, Y_val)\n",
    "print(\"Validation Accuracy:\", accuracy_val_rndforest)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores_rndforest = cross_val_score(model_rndforest, X_train, Y_train, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_rndforest)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_rndforest))\n",
    "\n",
    "# Evaluate additional metrics on the test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "accuracy_rndforest = model_rndforest.score(X_test, Y_test)\n",
    "Y_pred_rndforest = model_rndforest.predict(X_test)\n",
    "precision_rndforest = precision_score(Y_test, Y_pred_rndforest)\n",
    "recall_rndforest = recall_score(Y_test, Y_pred_rndforest)\n",
    "f1_rndforest = f1_score(Y_test, Y_pred_rndforest)\n",
    "roc_auc_rndforest = roc_auc_score(Y_test, Y_pred_rndforest)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_rndforest)\n",
    "print(\"Precision:\", precision_rndforest)\n",
    "print(\"Recall:\", recall_rndforest)\n",
    "print(\"F1-Score:\", f1_rndforest)\n",
    "print(\"ROC AUC Score:\", roc_auc_rndforest)\n",
    "\n",
    "# Save\n",
    "joblib.dump(model_rndforest, 'model_rndforest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113063,
     "status": "ok",
     "timestamp": 1751563128842,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "u6PaoDF4izhy",
    "outputId": "730252fe-5955-49af-d571-7950877b2b04"
   },
   "outputs": [],
   "source": [
    "# --- XAI Tools  RandomForest ---\n",
    "# IAL: Immediate action limits\n",
    "# IL:  Intervention limits\n",
    "\n",
    "# 1. LIME\n",
    "\n",
    "explainer_lime_rnd = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['IL', 'IAL'],\n",
    "    mode='classification'\n",
    ")\n",
    "i = 0  # instance\n",
    "exp_rnd = explainer_lime_rnd.explain_instance(X_test.iloc[i],model_rndforest.predict_proba)\n",
    "exp_rnd.show_in_notebook(show_table=True)\n",
    "\n",
    "joblib.dump(exp_rnd, \"explainer_lime_rnd.joblib\")\n",
    "\n",
    "# 2. SHAP\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer_shap_rnd = shap.Explainer(model_rndforest, X_train)\n",
    "\n",
    "# Compute SHAP values on the test set\n",
    "shap_values_rnd = explainer_shap_rnd(X_test[:1000], check_additivity=False)  #beacuse of scaling\n",
    "\n",
    "# Get SHAP values only for class 1 (assuming output is 3D)\n",
    "shap_values_class1 = shap_values_rnd[:, :, 1]\n",
    "\n",
    "#Now plot global importance\n",
    "shap.plots.waterfall(shap_values_class1[0])\n",
    "shap.plots.beeswarm(shap_values_class1, max_display=10)\n",
    "shap.plots.bar(shap_values_class1, max_display=10)\n",
    "shap.plots.heatmap(shap_values_class1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "id": "PPihmazh6t8p"
   },
   "outputs": [],
   "source": [
    "# Save LIME explanation as image\n",
    "fig_lime_rf = exp_rnd.as_pyplot_figure()\n",
    "fig_lime_rf.savefig(\"lime_explanation_rndforest.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_lime_rf)\n",
    "\n",
    "# Save SHAP beeswarm plot\n",
    "shap.plots.beeswarm(shap_values_class1, max_display=10, show=False)\n",
    "plt.savefig(\"shap_beeswarm_rndforest.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP waterfall plot for the first instance\n",
    "shap.plots.waterfall(shap_values_class1[0], show=False)\n",
    "plt.savefig(\"shap_waterfall_rndforest.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP bar plot\n",
    "shap.plots.bar(shap_values_class1, max_display=10, show=False)\n",
    "plt.savefig(\"shap_bar_rndforest.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP bar plot\n",
    "shap.plots.heatmap(shap_values_class1, max_display=10, show=False)\n",
    "plt.savefig(\"shap_heatmap_rndforest.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "65147de7-f47d-4690-9d71-c74da7848416"
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7088,
     "status": "ok",
     "timestamp": 1751563142498,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "425c29fa-3d50-4aba-a665-6e89fe831c17",
    "outputId": "b33c041e-c443-40ec-f95e-7f16edb15f2c"
   },
   "outputs": [],
   "source": [
    "#ML #Logistic Regression+SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "filtered_data_logistic = pd.read_excel(\"filtered_data_XAI.xlsx\")\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "categorical_columns = ['MONTH','DEF_AMPLTD', 'DEF_LGTH', 'TEST_FSPD', 'TEST_PSPD', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "filtered_data_logistic[categorical_columns] = filtered_data_logistic[categorical_columns].apply(pd.to_numeric)\n",
    "\n",
    "categorical_columns = ['DEF_PRTY', 'TSC_CD', 'DFCT_TYPE']\n",
    "for col in categorical_columns:\n",
    "    filtered_data_logistic[col] = filtered_data_logistic[col].replace({'YEL': 0, 'RED': 1, 'T': 0, 'C': 1, 'XLEVEL': 1, 'DIP': 2, 'SURFACE': 3})\n",
    "\n",
    "# Keep only data below the 98th percentile\n",
    "selected_cols_98 = ['DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "percentile_98 = filtered_data_logistic[selected_cols_98].quantile(0.98)\n",
    "filtered_data_logistic = filtered_data_logistic[\n",
    "    (filtered_data_logistic[selected_cols_98] <= percentile_98).all(axis=1)\n",
    "]\n",
    "\n",
    "# Define input features (X) and target variable (Y) for training\n",
    "#X_train = filtered_data_logistic[['MONTH','YEAR','DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']]\n",
    "#X_train =filtered_data_logistic[['MONTH','YEAR','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST']]\n",
    "X_raw = filtered_data_logistic[['DEF_LGTH','TEST_FSPD','TSC_CD','TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "#X_raw = filtered_data_logistic[['DEF_LGTH','TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "Y_train = filtered_data_logistic['DEF_PRTY']\n",
    "\n",
    "# Apply StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_raw), columns=X_raw.columns)\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Logistic Regression classifier\n",
    "model_logistic = LogisticRegression()\n",
    "\n",
    "# Train\n",
    "model_logistic.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "Y_val_pred = model_logistic.predict(X_val)\n",
    "accuracy_val_logistic = accuracy_score(Y_val, Y_val_pred)\n",
    "print(\"Validation Accuracy:\", accuracy_val_logistic)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores_logistic = cross_val_score(model_logistic, X_train, Y_train, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_logistic)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_logistic))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "Y_pred = model_logistic.predict(X_test)\n",
    "accuracy_test_logistic = accuracy_score(Y_test, Y_pred)\n",
    "precision_logistic = precision_score(Y_test, Y_pred)\n",
    "recall_logistic = recall_score(Y_test, Y_pred)\n",
    "f1_logistic = f1_score(Y_test, Y_pred)\n",
    "roc_auc_logistic = roc_auc_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_test_logistic)\n",
    "print(\"Precision:\", precision_logistic)\n",
    "print(\"Recall:\", recall_logistic)\n",
    "print(\"F1-Score:\", f1_logistic)\n",
    "print(\"ROC AUC Score:\", roc_auc_logistic)\n",
    "\n",
    "# Save\n",
    "import joblib\n",
    "joblib.dump(model_logistic, 'logistic_regression_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5712,
     "status": "ok",
     "timestamp": 1751563148217,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "uxRYHicSjM8w",
    "outputId": "58743785-ec77-4021-c46a-1574dcb2b958"
   },
   "outputs": [],
   "source": [
    "# --- XAI Tools  Logistic Regression ---\n",
    "# 1. LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# LIME expects the unscaled, raw input\n",
    "explainer_lime_log = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['IL', 'IAL'],\n",
    "    mode='classification'\n",
    ")\n",
    "i = 0  # instance\n",
    "exp_log = explainer_lime_log.explain_instance(X_test.iloc[i], model_logistic.predict_proba)\n",
    "exp_log.show_in_notebook(show_table=True)\n",
    "\n",
    "joblib.dump(exp_log, \"explainer_lime_log.joblib\")\n",
    "\n",
    "# 2. SHAP\n",
    "import shap\n",
    "\n",
    "# Use LinearExplainer for Logistic Regression\n",
    "explainer_shap_log = shap.Explainer(model_logistic, X_train)\n",
    "shap_values_log = explainer_shap_log(X_test[:1000])\n",
    "\n",
    "# Visualize SHAP values\n",
    "shap.plots.waterfall(shap_values_log[0])  # Local explanation\n",
    "shap.plots.beeswarm(shap_values_log)      # Global importance\n",
    "shap.plots.heatmap(shap_values_log)\n",
    "shap.plots.bar(shap_values_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "ZOcOhnlb9GGS"
   },
   "outputs": [],
   "source": [
    "# Save LIME plot\n",
    "fig_lime_log = exp_log.as_pyplot_figure()\n",
    "fig_lime_log.savefig(\"lime_explanation_logistic.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_lime_log)\n",
    "\n",
    "# Save SHAP beeswarm plot\n",
    "shap.plots.beeswarm(shap_values_log, show=False)\n",
    "plt.savefig(\"shap_beeswarm_logistic.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP waterfall plot\n",
    "shap.plots.waterfall(shap_values_log[0], show=False)\n",
    "plt.savefig(\"shap_waterfall_logistic.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP bar plot\n",
    "shap.plots.bar(shap_values_log, show=False)\n",
    "plt.savefig(\"shap_bar_logistic.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP heatmap plot\n",
    "shap.plots.heatmap(shap_values_log, show=False)\n",
    "plt.savefig(\"shap_heatmap_logistic.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "eaaf43ae-8630-4970-b473-1aa460f60508"
   },
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "deXRYCwBJ2nQ",
    "outputId": "384ad4f6-6df5-434c-9eb3-af17f33d2295"
   },
   "outputs": [],
   "source": [
    "#ML #SVM + SMOTE\n",
    "# Preparing whole data for training\n",
    "# Quantifying values in the \"DEF_PRTY\",\"TSC_CD\" and \"DFCT_TYPE\" column\n",
    "\n",
    "filtered_data_svm = pd.read_excel(\"filtered_data_XAI.xlsx\")\n",
    "filtered_data_svm['DEF_PRTY'] = filtered_data_svm['DEF_PRTY'].replace({'YEL': 0, 'RED': 1})\n",
    "filtered_data_svm['TSC_CD'] = filtered_data_svm['TSC_CD'].replace({'T': 0, 'C': 1})\n",
    "filtered_data_svm['DFCT_TYPE'] = filtered_data_svm['DFCT_TYPE'].replace({'XLEVEL': 1, 'DIP': 2, 'SURFACE': 3})\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#   outliers  percentile_9\n",
    "selected_cols_98 = ['DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "percentile_98 = filtered_data_svm[selected_cols_98].quantile(0.98)\n",
    "filtered_data_svm = filtered_data_svm[\n",
    "    (filtered_data_svm[selected_cols_98] <= percentile_98).all(axis=1)\n",
    "]\n",
    "\n",
    "# Features and target\n",
    "#X_train = filtered_data_svm[['MONTH','YEAR','DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']]\n",
    "X_raw = filtered_data_svm[['DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "#X_raw = filtered_data_svm[['DEF_LGTH','TEST_PSPD', 'DFCT_TYPE','TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "Y_train = filtered_data_svm['DEF_PRTY']\n",
    "\n",
    "# Apply StandardScaler and preserve feature names\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_raw), columns=X_raw.columns)\n",
    "\n",
    "# Split data to training and testing sets, 30% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# SVM Classifier\n",
    "model_svm = SVC(probability=True)\n",
    "\n",
    "# model training\n",
    "model_svm.fit(X_train, Y_train)\n",
    "\n",
    "# Validation accuracy\n",
    "accuracy_val_svm = model_svm.score(X_val, Y_val)\n",
    "print(\"Validation Accuracy:\", accuracy_val_svm)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_scores_svm = cross_val_score(model_svm, X_train, Y_train, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_svm)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_svm))\n",
    "\n",
    "# Evaluate the model\n",
    "Y_pred_svm = model_svm.predict(X_test)\n",
    "accuracy_test_svm = accuracy_score(Y_test, Y_pred_svm)\n",
    "precision_svm = precision_score(Y_test, Y_pred_svm)\n",
    "recall_svm = recall_score(Y_test, Y_pred_svm)\n",
    "f1_svm = f1_score(Y_test, Y_pred_svm)\n",
    "roc_auc_svm = roc_auc_score(Y_test, Y_pred_svm)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_test_svm)\n",
    "print(\"Precision:\", precision_svm)\n",
    "print(\"Recall:\", recall_svm)\n",
    "print(\"F1-Score:\", f1_svm)\n",
    "print(\"ROC AUC Score:\", roc_auc_svm)\n",
    "\n",
    "# Save\n",
    "import joblib\n",
    "joblib.dump(model_svm, 'model_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10620897,
     "status": "ok",
     "timestamp": 1751574005569,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "riOAk282J6-3",
    "outputId": "193460a7-8a21-4e8b-d4ee-347f9258ef08"
   },
   "outputs": [],
   "source": [
    "# --- XAI Tools SVM  ---\n",
    "# IAL: Immediate action limits\n",
    "# IL:  Intervention limits\n",
    "\n",
    "# 1. LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer_lime_svm= lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['IL','IA'],\n",
    "     mode='classification')\n",
    "\n",
    "i = 0  # instance number\n",
    "exp_svm = explainer_lime_svm.explain_instance(X_test.iloc[i], model_svm.predict_proba)\n",
    "exp_svm.show_in_notebook(show_table=True)\n",
    "\n",
    "joblib.dump(exp_svm, \"lime_explanation_svm.joblib\")\n",
    "\n",
    "# 2. SHAP\n",
    "import shap\n",
    "\n",
    "explainer_shap_svm = shap.Explainer(model_svm.predict, X_train)\n",
    "shap_values_svm = explainer_shap_svm(X_test[:1000]) # reduce X_test[n] to 10 for quick result\n",
    "\n",
    "shap.plots.waterfall(shap_values_svm[0])  # Local explanation\n",
    "shap.plots.beeswarm(shap_values_svm)      # Global importance\n",
    "shap.plots.heatmap(shap_values_svm)\n",
    "shap.plots.bar(shap_values_svm)\n",
    "\n",
    "\n",
    "# Saving explanation for furthurte use\n",
    "\n",
    "# --- Save SHAP Values ---\n",
    "joblib.dump(shap_values_svm, \"shap_values_svm.joblib\")  # Already correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "executionInfo": {
     "elapsed": 2322,
     "status": "ok",
     "timestamp": 1751574007920,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "eSPjX7Hm-mZD"
   },
   "outputs": [],
   "source": [
    "# --- Load LIME Explanation ---\n",
    "#exp_svm = joblib.load(\"lime_explanation_svm.joblib\")\n",
    "#exp_svm.show_in_notebook(show_table=True)  # Regenerate plot\n",
    "\n",
    "# --- Load SHAP Values ---\n",
    "#shap_values_svm = joblib.load(\"shap_values_svm.joblib\")\n",
    "#shap.plots.waterfall(shap_values_svm[0])  # Regenerate SHAP plots\n",
    "\n",
    "# Save LIME plot\n",
    "fig_lime_svm = exp_svm.as_pyplot_figure()\n",
    "fig_lime_svm.savefig(\"explainer_lime_svm.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_lime_svm)\n",
    "\n",
    "# Save SHAP beeswarm plot\n",
    "shap.plots.beeswarm(shap_values_svm, show=False)\n",
    "plt.savefig(\"shap_beeswarm_svm.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP waterfall plot\n",
    "shap.plots.waterfall(shap_values_svm[0], show=False)\n",
    "plt.savefig(\"shap_waterfall_svm.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP bar plot\n",
    "shap.plots.bar(shap_values_svm, show=False)\n",
    "plt.savefig(\"shap_bar_svm.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP heatmap plot\n",
    "shap.plots.heatmap(shap_values_svm, show=False)\n",
    "plt.savefig(\"shap_heatmap_svm.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "9d1122be-d759-4e1b-924d-dbb598836bdb"
   },
   "source": [
    "Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44408,
     "status": "ok",
     "timestamp": 1751574837905,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "96edc758-bbaa-4ed2-81f9-6040867bbf4c",
    "outputId": "1e059d3f-6dfb-4069-cc8f-f91c80481796"
   },
   "outputs": [],
   "source": [
    "# ML: Artificial Neural Network with SMOTE, Standardization, and XAI\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the data\n",
    "filtered_data_nn = pd.read_excel(\"filtered_data_XAI.xlsx\")\n",
    "filtered_data_nn['DEF_PRTY'] = filtered_data_nn['DEF_PRTY'].replace({'YEL': 0, 'RED': 1})\n",
    "filtered_data_nn['TSC_CD'] = filtered_data_nn['TSC_CD'].replace({'T': 0, 'C': 1})\n",
    "filtered_data_nn['DFCT_TYPE'] = filtered_data_nn['DFCT_TYPE'].replace({'XLEVEL': 1, 'DIP': 2, 'SURFACE': 3})\n",
    "\n",
    "# Keep only data below the 80th percentile for each numeric column to remove outliers\n",
    "selected_cols_98 = ['DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "percentile_98 = filtered_data_nn[selected_cols_98].quantile(0.98)\n",
    "filtered_data_nn = filtered_data_nn[\n",
    "    (filtered_data_nn[selected_cols_98] <= percentile_98).all(axis=1)\n",
    "]\n",
    "\n",
    "# Define features and target\n",
    "feature_names = ['DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST','TOT_DFLT_MGT']\n",
    "#feature_names = ['DEF_LGTH','TEST_PSPD', 'DFCT_TYPE','TOT_CAR_WEST','TOT_DFLT_MGT']\n",
    "X_train = filtered_data_nn[feature_names]\n",
    "Y_train = filtered_data_nn['DEF_PRTY']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Initialize and train MLPClassifier\n",
    "model_nn = MLPClassifier(hidden_layer_sizes=(35), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "model_nn.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "# Validation accuracy\n",
    "accuracy_val_nn = model_nn.score(X_val, Y_val)\n",
    "print(\"Validation Accuracy:\", accuracy_val_nn)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores_nn = cross_val_score(model_nn, X_train_resampled, Y_train_resampled, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_nn)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_nn))\n",
    "\n",
    "# Evaluate on test set\n",
    "accuracy_test_nn = model_nn.score(X_test, Y_test)\n",
    "precision_nn = precision_score(Y_test, model_nn.predict(X_test))\n",
    "recall_nn = recall_score(Y_test, model_nn.predict(X_test))\n",
    "f1_nn = f1_score(Y_test, model_nn.predict(X_test))\n",
    "roc_auc_nn = roc_auc_score(Y_test, model_nn.predict(X_test))\n",
    "\n",
    "print(\"\\nTest Performance Metrics:\")\n",
    "print(\"Test Accuracy:\", accuracy_test_nn)\n",
    "print(\"Precision:\", precision_nn)\n",
    "print(\"Recall:\", recall_nn)\n",
    "print(\"F1-Score:\", f1_nn)\n",
    "print(\"ROC AUC Score:\", roc_auc_nn)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model_nn, 'neural_network_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3e8e22b7f6a940b3a471381c3b7d994f",
      "a7eab39cec3d4affbc28cb48428afdc2",
      "1877afef638b48cfbe05aa8ca9dec255",
      "b5877863679e4991a8536cdf4065ef6e",
      "f486324ce1b443abbb1358db7fe51ce9",
      "ffc1b7b601734e55a08cb1fdd31a20f0",
      "751c427faffb4b9ba8c4215695fba5a9",
      "21ba8eef9b4046be9390a18edd9afffe",
      "7ce00addff5b466f9edfef30738a4846",
      "5f60dab32f6d4cbdafe25d98a25b146c",
      "2a45b838a2e342e3bfcde3476e0a06e6"
     ]
    },
    "executionInfo": {
     "elapsed": 94899,
     "status": "ok",
     "timestamp": 1751575004593,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "e5NRykC2AAVv",
    "outputId": "d5a5beb6-aabf-45fe-ff74-f1da6234c115"
   },
   "outputs": [],
   "source": [
    "# --- XAI Tools  Neural Network---\n",
    "# IAL: Immediate action limits\n",
    "# IL: Intervention limits\n",
    "\n",
    "# 1. LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer_lime_nn = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train_resampled),\n",
    "    feature_names=feature_names,\n",
    "    class_names=['IL','IA'],\n",
    "    mode='classification')\n",
    "\n",
    "i = 0  # instance number\n",
    "exp_nn = explainer_lime_nn.explain_instance(X_test[i], model_nn.predict_proba)\n",
    "exp_nn.show_in_notebook(show_table=True)\n",
    "\n",
    "joblib.dump(exp_svm, \"explainer_lime_nn.joblib\")\n",
    "\n",
    "# 2. SHAP\n",
    "import shap\n",
    "\n",
    "explainer_shap_nn = shap.KernelExplainer(model_nn.predict, X_train[:1000])\n",
    "shap_values_array = explainer_shap_nn.shap_values(X_test[:1000])\n",
    "shap_values_nn = shap.Explanation(values=shap_values_array,\n",
    "                                  base_values=np.repeat(explainer_shap_nn.expected_value, X_test[:1000].shape[0]),\n",
    "                                  data=X_test[:1000],\n",
    "                                  feature_names=feature_names)\n",
    "\n",
    "# Visualize SHAP values\n",
    "shap.plots.waterfall(shap_values_nn[0])  # Local explanation\n",
    "shap.plots.beeswarm(shap_values_nn)      # Global importance\n",
    "shap.plots.heatmap(shap_values_nn)\n",
    "shap.plots.bar(shap_values_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "executionInfo": {
     "elapsed": 3288,
     "status": "ok",
     "timestamp": 1751575007901,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "Myqu3yxxKajn"
   },
   "outputs": [],
   "source": [
    "# Save LIME explanation as image\n",
    "fig_lime_nn = exp_nn.as_pyplot_figure()\n",
    "fig_lime_nn.savefig(\"lime_explanation_nn.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_lime_nn)\n",
    "\n",
    "# Save SHAP beeswarm plot\n",
    "shap.plots.beeswarm(shap_values_nn, show=False)\n",
    "plt.savefig(\"shap_beeswarm_nn.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Save SHAP waterfall plot\n",
    "shap.plots.waterfall(shap_values_nn[0], show=False)\n",
    "plt.savefig(\"shap_waterfall_nn.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP bar plot\n",
    "shap.plots.bar(shap_values_nn, show=False)\n",
    "plt.savefig(\"shap_bar_nn.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP heatmap plot\n",
    "shap.plots.heatmap(shap_values_nn, show=False)\n",
    "plt.savefig(\"shap_heatmap_nn.jpeg\", format=\"jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1751575048897,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "ggvrey4gAiP9",
    "outputId": "9e029c08-d8c6-4009-dade-3ef46835f62d"
   },
   "outputs": [],
   "source": [
    "#ML accuracy & validation results :\n",
    "\n",
    "print(\"Gradient Boosting Model:\")\n",
    "print(\"Validation Accuracy:\", accuracy_test_grboost)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_grboost)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_grboost))\n",
    "print(\"Test Accuracy:\", accuracy_test_grboost)\n",
    "print(\"Precision:\", precision_grboost)\n",
    "print(\"Recall:\", recall_grboost)\n",
    "print(\"F1-Score:\", f1_grboost)\n",
    "print(\"ROC AUC Score:\", roc_auc_grboost)\n",
    "print()\n",
    "\n",
    "print(\"XGBOOST Model:\")\n",
    "print(\"Validation Accuracy:\", accuracy_test_xgboost)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_xgboost)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_xgboost))\n",
    "print(\"Test Accuracy:\", accuracy_test_xgboost)\n",
    "print(\"Precision:\", precision_xgboost)\n",
    "print(\"Recall:\", recall_xgboost)\n",
    "print(\"F1-Score:\", f1_xgboost)\n",
    "print(\"ROC AUC Score:\", roc_auc_xgboost)\n",
    "print()\n",
    "\n",
    "print(\"Random forest Model:\")\n",
    "print(\"Validation Accuracy:\", accuracy_rndforest)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_rndforest)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_rndforest))\n",
    "print(\"Test Accuracy:\", accuracy_rndforest)\n",
    "print(\"Precision:\", precision_rndforest)\n",
    "print(\"Recall:\", recall_rndforest)\n",
    "print(\"F1-Score:\", f1_rndforest)\n",
    "print(\"ROC AUC Score:\", roc_auc_rndforest)\n",
    "print()\n",
    "\n",
    "print(\"Logistic Regression Model:\")\n",
    "print(\"Validation Accuracy:\", accuracy_val_logistic)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_logistic)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_logistic))\n",
    "print(\"Test Accuracy:\", accuracy_test_logistic)\n",
    "print(\"Precision:\", precision_logistic)\n",
    "print(\"Recall:\", recall_logistic)\n",
    "print(\"F1-Score:\", f1_logistic)\n",
    "print(\"ROC AUC Score:\", roc_auc_logistic)\n",
    "print()\n",
    "\n",
    "print(\"Support Vector Machine Model:\")\n",
    "print(\"Validation Accuracy:\", accuracy_val_svm)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_svm)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_svm))\n",
    "print(\"Test Accuracy:\", accuracy_test_svm)\n",
    "print(\"Precision:\", precision_svm)\n",
    "print(\"Recall:\", recall_svm)\n",
    "print(\"F1-Score:\", f1_svm)\n",
    "print(\"ROC AUC Score:\", roc_auc_svm)\n",
    "print()\n",
    "\n",
    "print(\"Neural Network Model:\")\n",
    "print(\"Validation Accuracy:\", accuracy_test_nn)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_nn)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_nn))\n",
    "print(\"Test Accuracy:\", accuracy_test_nn)\n",
    "print(\"Precision:\", precision_nn)\n",
    "print(\"Recall:\", recall_nn)\n",
    "print(\"F1-Score:\", f1_nn)\n",
    "print(\"ROC AUC Score:\", roc_auc_nn)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1751575062485,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "1UYPSq6yAl11",
    "outputId": "53180ff7-1588-467c-a67b-f2387a11d4ba"
   },
   "outputs": [],
   "source": [
    "# table for comparing ML:\n",
    "\n",
    "# Data for the table\n",
    "ml_table = [\n",
    "    [\"Logistic Regression\", accuracy_val_logistic, np.mean(cv_scores_logistic), accuracy_test_logistic, precision_logistic, recall_logistic, f1_logistic, roc_auc_logistic],\n",
    "    [\"Gradient Boosting\", accuracy_val_grboost, np.mean(cv_scores_grboost), accuracy_test_grboost, precision_grboost, recall_grboost, f1_grboost, roc_auc_grboost],\n",
    "    [\"XGBOOST\", accuracy_val_xgboost, np.mean(cv_scores_xgboost), accuracy_test_xgboost, precision_xgboost, recall_xgboost, f1_xgboost, roc_auc_xgboost],\n",
    "    [\"Random Forest\", accuracy_val_rndforest, np.mean(cv_scores_rndforest), accuracy_rndforest, precision_rndforest, recall_rndforest, f1_rndforest, roc_auc_rndforest],\n",
    "    [\"SVM\", accuracy_val_svm,  np.mean(cv_scores_svm), accuracy_test_svm, precision_svm, recall_svm, f1_svm, roc_auc_svm],\n",
    "    [\"Neural Network\", accuracy_val_nn,np.mean(cv_scores_nn), accuracy_test_nn, precision_nn, recall_nn, f1_nn, roc_auc_nn],]\n",
    "\n",
    "# Column headers\n",
    "headers = [\"Model\", \"Validation Accuracy\", \"Mean Cross-Validation Score\", \"Test Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC AUC Score\"]\n",
    "\n",
    "# Convert the ML table into a DataFrame\n",
    "ml_table = pd.DataFrame(ml_table, columns=headers)\n",
    "\n",
    "display(ml_table)\n",
    "ml_table.to_excel(\"ml_table.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8745,
     "status": "ok",
     "timestamp": 1751575016651,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "baba709c-72cc-4e3c-9319-077c2cbea4ab",
    "outputId": "b3b6cc01-b4bd-4fbd-e050-b90f255f6ede"
   },
   "outputs": [],
   "source": [
    "\n",
    "display('Logistic Regression')\n",
    "\n",
    "shap.plots.beeswarm(shap_values_log)\n",
    "\n",
    "display('GRboost')\n",
    "\n",
    "shap.plots.beeswarm(shap_values_grboost)\n",
    "\n",
    "display('XGboost')\n",
    "\n",
    "shap.plots.beeswarm(shap_values_xgb)\n",
    "\n",
    "display('RNDforest')\n",
    "\n",
    "shap.plots.beeswarm(shap_values_class1)\n",
    "\n",
    "display('Support Vector Machine')\n",
    "\n",
    "shap.plots.beeswarm(shap_values_svm)\n",
    "\n",
    "display('Neural Network')\n",
    "\n",
    "shap.plots.beeswarm(shap_values_nn[:1000])  #Always check the number [:100]\n",
    "\n",
    "shap_values_nn = shap.Explanation(\n",
    "    values=shap_values_array,\n",
    "    base_values=np.repeat(explainer_shap_nn.expected_value, X_test[:1000].shape[0]),\n",
    "    data=X_test[:1000],\n",
    "    feature_names=feature_names\n",
    ")\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Titles and corresponding SHAP values\n",
    "titles = [\n",
    "    'Logistic Regression',\n",
    "    'GRboost',\n",
    "    'XGboost',\n",
    "    'Random Forest',\n",
    "    'Support Vector Machine',\n",
    "    'Neural Network'\n",
    "]\n",
    "\n",
    "shap_values_list = [\n",
    "    shap_values_log,\n",
    "    shap_values_grboost,\n",
    "    shap_values_xgb,\n",
    "    shap_values_class1,     # or shap_values_rnd if preferred\n",
    "    shap_values_svm,\n",
    "    shap_values_nn\n",
    "]\n",
    "\n",
    "\n",
    "# Save each SHAP beeswarm plot individually\n",
    "filepaths = []\n",
    "for title, shap_vals in zip(titles, shap_values_list):\n",
    "    shap.summary_plot(shap_vals, show=False, plot_type='dot', plot_size=(12, 6))\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    filepath = f\"{title.replace(' ', '_')}_shap.png\"\n",
    "    fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    filepaths.append(filepath)\n",
    "    plt.close()\n",
    "\n",
    "# Combine saved images vertically into one\n",
    "images = [Image.open(fp) for fp in filepaths]\n",
    "total_height = sum(img.height for img in images)\n",
    "max_width = max(img.width for img in images)\n",
    "combined = Image.new('RGB', (max_width, total_height), (255, 255, 255))\n",
    "\n",
    "y_offset = 0\n",
    "for img in images:\n",
    "    combined.paste(img, (0, y_offset))\n",
    "    y_offset += img.height\n",
    "\n",
    "combined.save(\"SHAP_beeswarm_all_models.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3937,
     "status": "ok",
     "timestamp": 1751575020615,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "iuSKMQAR66sM",
    "outputId": "4d9e259e-0ef2-4fd7-b572-75f3cd27bcf9"
   },
   "outputs": [],
   "source": [
    "# Create folder for temporary plots\n",
    "os.makedirs(\"lime_temp_plots\", exist_ok=True)\n",
    "\n",
    "# Load LIME explanations\n",
    "lime_exps = {\n",
    "    \"Logistic Regression\": joblib.load(\"explainer_lime_log.joblib\"),\n",
    "    \"Random Forest\": joblib.load(\"explainer_lime_rnd.joblib\"),\n",
    "    \"Gradient Boosting\": joblib.load(\"explainer_lime_grboost.joblib\"),\n",
    "    \"XGBoost\": joblib.load(\"explainer_lime_xgb.joblib\"),\n",
    "    \"SVM\": joblib.load(\"lime_explanation_svm.joblib\"),\n",
    "    \"Neural Network\": joblib.load(\"explainer_lime_nn.joblib\")\n",
    "}\n",
    "\n",
    "# Instance setup\n",
    "instance_index = 10\n",
    "\n",
    "# Ensure instance_data is 1D array\n",
    "instance_data = X_train[instance_index]\n",
    "instance_data = instance_data.flatten() if instance_data.ndim > 1 else instance_data\n",
    "\n",
    "# Access class label\n",
    "class_label = Y_train.iloc[instance_index] if hasattr(Y_train, \"iloc\") else Y_train[instance_index]\n",
    "\n",
    "# Feature names (adjust to match your data)\n",
    "feature_names = ['DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST','TOT_DFLT_MGT']\n",
    "\n",
    "# Validate length\n",
    "assert len(instance_data) == len(feature_names), \"Mismatch between instance and feature names\"\n",
    "\n",
    "# Build feature table\n",
    "feature_table = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Value': instance_data\n",
    "})\n",
    "\n",
    "# --- Plot setup ---\n",
    "fig = plt.figure(figsize=(8, 26))\n",
    "gs = fig.add_gridspec(7, 2, height_ratios=[0.9] + [1]*6)\n",
    "\n",
    "# Row 0: Table at top\n",
    "ax_table = fig.add_subplot(gs[0, :])\n",
    "ax_table.axis(\"off\")\n",
    "table = ax_table.table(\n",
    "    cellText=feature_table.values,\n",
    "    colLabels=feature_table.columns,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(8)\n",
    "table.scale(1.1, 0.6)\n",
    "ax_table.set_title(f\"Instance {instance_index}  True Class: {class_label}\", fontsize=14, pad=12)\n",
    "\n",
    "# Rows 16: LIME plots\n",
    "for i, (model_name, exp) in enumerate(lime_exps.items()):\n",
    "    ax = fig.add_subplot(gs[i // 2 + 1, i % 2])\n",
    "\n",
    "    # Generate LIME plot and customize font sizes\n",
    "    fig_lime = exp.as_pyplot_figure(label=1)\n",
    "    ax_lime = fig_lime.axes[0]\n",
    "    ax_lime.tick_params(axis='both', labelsize=12)\n",
    "    ax_lime.set_xlabel(ax_lime.get_xlabel(), fontsize=12)\n",
    "    ax_lime.set_ylabel(ax_lime.get_ylabel(), fontsize=12)\n",
    "    fig_lime.tight_layout()\n",
    "\n",
    "    # Save and reinsert into the main grid\n",
    "    image_path = f\"lime_temp_plots/{model_name.replace(' ', '_')}.png\"\n",
    "    fig_lime.savefig(image_path, bbox_inches='tight')\n",
    "    plt.close(fig_lime)\n",
    "\n",
    "    img = plt.imread(image_path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(model_name, fontsize=12)\n",
    "\n",
    "# Final layout\n",
    "plt.suptitle(f\"LIME Explanations for Instance {instance_index}\", fontsize=30, y=0.96)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"lime_comparison_grid_with_top_table.png\", dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1751575020668,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "TBH5XYhQxE0a",
    "outputId": "5b8acc2b-d9ae-4756-e1a9-f7bd938e713e"
   },
   "outputs": [],
   "source": [
    "# Instance display\n",
    "\n",
    "\n",
    "\n",
    "feature_names = ['DEF_LGTH', 'TSC_CD', 'TEST_FSPD', 'TEST_PSPD',\n",
    "                 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_DFLT_MGT']\n",
    "\n",
    "# Extract instance\n",
    "instance_o = filtered_data_grboost.iloc[instance_index][feature_names]\n",
    "\n",
    "#  table\n",
    "instance_table = pd.DataFrame(instance_o).T  # Transpose to show as single row table\n",
    "\n",
    "print(instance_table.to_string(index=False))  # index=False removes row numbering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "l0CFO6Zh7Eof"
   },
   "source": [
    "SVM Second run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162192,
     "status": "ok",
     "timestamp": 1751576023937,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "1T8GotLRBQG8",
    "outputId": "e56f0004-56df-41c9-df46-cb5838b23754"
   },
   "outputs": [],
   "source": [
    "#ML #SVM + SMOTE   SECOND RUN WITHOUT 'TOT_CAR_EAST' and 'TSC_CD'\n",
    "# Preparing whole data for training\n",
    "# Quantifying values in the \"DEF_PRTY\",\"TSC_CD\" and \"DFCT_TYPE\" column\n",
    "\n",
    "filtered_data_svm2 = pd.read_excel(\"filtered_data_XAI.xlsx\")\n",
    "filtered_data_svm2['DEF_PRTY'] = filtered_data_svm2['DEF_PRTY'].replace({'YEL': 0, 'RED': 1})\n",
    "filtered_data_svm2['TSC_CD'] = filtered_data_svm2['TSC_CD'].replace({'T': 0, 'C': 1})\n",
    "filtered_data_svm2['DFCT_TYPE'] = filtered_data_svm2['DFCT_TYPE'].replace({'XLEVEL': 1, 'DIP': 2, 'SURFACE': 3})\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#   outliers  percentile_9\n",
    "selected_cols_98 = ['DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']\n",
    "percentile_98 = filtered_data_svm2[selected_cols_98].quantile(0.98)\n",
    "filtered_data_svm2 = filtered_data_svm2[\n",
    "    (filtered_data_svm2[selected_cols_98] <= percentile_98).all(axis=1)\n",
    "]\n",
    "\n",
    "# Features and target\n",
    "#X_train = filtered_data_svm2[['MONTH','YEAR','DEF_AMPLTD','DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST', 'TOT_TRN_EAST', 'TOT_TRN_WEST', 'TOT_DFLT_MGT']]\n",
    "#X_raw = filtered_data_svm2[['DEF_LGTH','TSC_CD','TEST_FSPD', 'TEST_PSPD', 'DFCT_TYPE', 'TOT_CAR_EAST', 'TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "X_raw = filtered_data_svm2[['DEF_LGTH','TEST_PSPD', 'DFCT_TYPE','TOT_CAR_WEST','TOT_DFLT_MGT']]\n",
    "Y_train = filtered_data_svm2['DEF_PRTY']\n",
    "\n",
    "# Apply StandardScaler and preserve feature names\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_raw), columns=X_raw.columns)\n",
    "\n",
    "# Split data to training and testing sets, 30% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# SVM Classifier\n",
    "model_svm2 = SVC(probability=True)\n",
    "\n",
    "# model training\n",
    "model_svm2.fit(X_train, Y_train)\n",
    "\n",
    "# Validation accuracy\n",
    "accuracy_val_svm2 = model_svm2.score(X_val, Y_val)\n",
    "print(\"Validation Accuracy:\", accuracy_val_svm2)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_scores_svm2 = cross_val_score(model_svm2, X_train, Y_train, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_svm2)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_svm2))\n",
    "\n",
    "# Evaluate the model\n",
    "Y_pred_svm2 = model_svm2.predict(X_test)\n",
    "accuracy_test_svm2 = accuracy_score(Y_test, Y_pred_svm2)\n",
    "precision_svm2 = precision_score(Y_test, Y_pred_svm2)\n",
    "recall_svm2 = recall_score(Y_test, Y_pred_svm2)\n",
    "f1_svm2 = f1_score(Y_test, Y_pred_svm2)\n",
    "roc_auc_svm2 = roc_auc_score(Y_test, Y_pred_svm2)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_test_svm2)\n",
    "print(\"Precision:\", precision_svm2)\n",
    "print(\"Recall:\", recall_svm2)\n",
    "print(\"F1-Score:\", f1_svm2)\n",
    "print(\"ROC AUC Score:\", roc_auc_svm2)\n",
    "\n",
    "# Save\n",
    "import joblib\n",
    "joblib.dump(model_svm2, 'model_svm2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "executionInfo": {
     "elapsed": 16858,
     "status": "ok",
     "timestamp": 1751576040827,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "qOmIz8ro7KVX",
    "outputId": "789a730f-f2da-4271-bbd9-36b731a5be1a"
   },
   "outputs": [],
   "source": [
    "# --- XAI Tools SVM  ---\n",
    "# IAL: Immediate action limits\n",
    "# IL:  Intervention limits\n",
    "\n",
    "# 1. LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer_lime_svm2= lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['IL','IA'],\n",
    "     mode='classification')\n",
    "\n",
    "i = 0  # instance number\n",
    "exp_svm2 = explainer_lime_svm2.explain_instance(X_test.iloc[i], model_svm2.predict_proba)\n",
    "exp_svm2.show_in_notebook(show_table=True)\n",
    "\n",
    "joblib.dump(exp_svm2, \"lime_explanation_svm2.joblib\")\n",
    "\n",
    "# 2. SHAP\n",
    "import shap\n",
    "\n",
    "explainer_shap_svm2 = shap.Explainer(model_svm2.predict, X_train)\n",
    "shap_values_svm2 = explainer_shap_svm2(X_test[:10]) # increase X_test[n] to 1000 for better result\n",
    "\n",
    "#shap.plots.waterfall(shap_values_svm2[0])  # Local explanation\n",
    "#shap.plots.beeswarm(shap_values_svm2)      # Global importance\n",
    "shap.plots.heatmap(shap_values_svm2)\n",
    "#shap.plots.bar(shap_values_svm2)\n",
    "\n",
    "\n",
    "# Saving explanation for furthurte use\n",
    "\n",
    "# --- Save SHAP Values ---\n",
    "joblib.dump(shap_values_svm2, \"shap_values_svm2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "executionInfo": {
     "elapsed": 2051,
     "status": "ok",
     "timestamp": 1751576042895,
     "user": {
      "displayName": "Shahab Aldin Mansouri",
      "userId": "17815084975420395517"
     },
     "user_tz": 240
    },
    "id": "d9o38nvC7QZH"
   },
   "outputs": [],
   "source": [
    "# --- Load LIME Explanation ---\n",
    "#exp_svm2 = joblib.load(\"lime_explanation_svm2.joblib\")\n",
    "#exp_svm2.show_in_notebook(show_table=True)  # Regenerate plot\n",
    "\n",
    "# --- Load SHAP Values ---\n",
    "#shap_values_svm2 = joblib.load(\"shap_values_svm2.joblib\")\n",
    "#shap.plots.waterfall(shap_values_svm2[0])  # Regenerate SHAP plots\n",
    "\n",
    "# Save LIME plot\n",
    "fig_lime_svm2 = exp_svm2.as_pyplot_figure()\n",
    "fig_lime_svm2.savefig(\"explainer_lime_svm2.jpeg\", format=\"jpeg\", dpi=600, bbox_inches='tight')\n",
    "plt.close(fig_lime_svm2)\n",
    "\n",
    "# Save SHAP beeswarm plot\n",
    "shap.plots.beeswarm(shap_values_svm2, show=False)\n",
    "plt.savefig(\"shap_beeswarm_svm2.jpeg\", format=\"jpeg\", dpi=600, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP waterfall plot\n",
    "shap.plots.waterfall(shap_values_svm2[0], show=False)\n",
    "plt.savefig(\"shap_waterfall_svm2.jpeg\", format=\"jpeg\", dpi=600, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP bar plot\n",
    "shap.plots.bar(shap_values_svm2, show=False)\n",
    "plt.savefig(\"shap_bar_svm2.jpeg\", format=\"jpeg\", dpi=600, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save SHAP heatmap plot\n",
    "shap.plots.heatmap(shap_values_svm2, show=False)\n",
    "plt.savefig(\"shap_heatmap_svm2.jpeg\", format=\"jpeg\", dpi=600, bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1nPL_nUAseZDqyir9JSJtwlAZEeHwR7S9",
     "timestamp": 1750991102388
    },
    {
     "file_id": "1RlSdcyFGlcnUK49lx4dyb5Vkgsp8fcRp",
     "timestamp": 1750280107573
    },
    {
     "file_id": "1-nkfCFq_hOqxmuNtwm6k3HguiBRufYsN",
     "timestamp": 1750103166037
    },
    {
     "file_id": "1vmFoZVbJVTV_rDwsxlxe7UXow7nvsJQj",
     "timestamp": 1750100865376
    },
    {
     "file_id": "1ccGD4IYQzJqgosnDu2UEijVyKP2B4TN1",
     "timestamp": 1749848461774
    },
    {
     "file_id": "1w6ftf5SbIbI0Y4QOTmFcaySb1_DS9Xo3",
     "timestamp": 1749665627666
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
